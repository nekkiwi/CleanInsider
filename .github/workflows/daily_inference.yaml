name: Daily Inference and Trading

on:
  # Run at 10:00 AM ET (15:00 UTC) on weekdays - after market opens
  schedule:
    - cron: '0 15 * * 1-5'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'Dry run (no actual trades)'
        required: false
        default: 'false'
        type: boolean
      days_back:
        description: 'Days to look back for events'
        required: false
        default: '7'
        type: string

permissions:
  contents: read

env:
  # Alpaca credentials
  ALPACA_API_KEY: ${{ secrets.ALPACA_API_KEY }}
  ALPACA_SECRET_KEY: ${{ secrets.ALPACA_SECRET_KEY }}
  PAPER_MODE: 'true'
  
  # Google credentials
  GOOGLE_DRIVE_CREDENTIALS: ${{ secrets.GOOGLE_DRIVE_CREDENTIALS }}
  GDRIVE_MODELS_FOLDER_ID: ${{ secrets.GDRIVE_MODELS_FOLDER_ID }}
  GDRIVE_LOG_SHEET_ID: ${{ secrets.GDRIVE_LOG_SHEET_ID }}

jobs:
  inference:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        model:
          - id: model_1w_tp5_sl5
            folder: model_1w_tp5_sl5
            strategy: 1w_tp0p05_sl-0p05
          - id: model_2w_tp5_sl5
            folder: model_2w_tp5_sl5
            strategy: 2w_tp0p05_sl-0p05
          - id: model_1m_tp5_sl5
            folder: model_1m_tp5_sl5
            strategy: 1m_tp0p05_sl-0p05
    
    name: "${{ matrix.model.id }}"
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Install rclone
        run: |
          curl https://rclone.org/install.sh | sudo bash
      
      - name: Create directories
        run: |
          mkdir -p data/models/${{ matrix.model.strategy }}
          mkdir -p data/scrapers/features/preprocessing
          mkdir -p data/scrapers/features/components
          mkdir -p logs/trades
          mkdir -p logs/performance
      
      - name: Download models from Google Drive
        env:
          RCLONE_CONFIG_GDRIVE_TYPE: drive
          RCLONE_CONFIG_GDRIVE_SCOPE: drive.readonly
          RCLONE_CONFIG_GDRIVE_SERVICE_ACCOUNT_CREDENTIALS: ${{ secrets.GOOGLE_DRIVE_CREDENTIALS }}
        run: |
          echo "Downloading model: ${{ matrix.model.folder }}"
          
          # Download weights and reconstruct folder structure
          echo "Downloading weights..."
          rclone copy "gdrive,root_folder_id=${{ secrets.GDRIVE_MODELS_FOLDER_ID }}:${{ matrix.model.folder }}/weights/" "data/models/${{ matrix.model.strategy }}_flat/" -v
          
          # Reconstruct fold/seed structure from flat files
          echo "Reconstructing folder structure..."
          for file in data/models/${{ matrix.model.strategy }}_flat/*.pkl; do
            if [ -f "$file" ]; then
              filename=$(basename "$file")
              # Parse fold1_seed42_classifier.pkl -> fold_1/seed_42/classifier.pkl
              if [[ $filename =~ ^fold([0-9]+)_seed([0-9]+)_(.+)$ ]]; then
                fold="${BASH_REMATCH[1]}"
                seed="${BASH_REMATCH[2]}"
                rest="${BASH_REMATCH[3]}"
                mkdir -p "data/models/${{ matrix.model.strategy }}/fold_${fold}/seed_${seed}"
                cp "$file" "data/models/${{ matrix.model.strategy }}/fold_${fold}/seed_${seed}/${rest}"
                echo "  $filename -> fold_${fold}/seed_${seed}/${rest}"
              fi
            fi
          done
          
          # Download preprocessing artifacts
          echo "Downloading preprocessing..."
          rclone copy "gdrive,root_folder_id=${{ secrets.GDRIVE_MODELS_FOLDER_ID }}:${{ matrix.model.folder }}/preprocessing/" "data/scrapers/features/preprocessing/" -v
          
          echo "Download complete."
          ls -la data/models/${{ matrix.model.strategy }}/
          ls -la data/scrapers/features/preprocessing/
      
      - name: Run inference for ${{ matrix.model.id }}
        run: |
          ARGS="--model ${{ matrix.model.id }}"
          
          if [ "${{ inputs.dry_run }}" = "true" ]; then
            ARGS="$ARGS --dry-run"
          fi
          
          if [ -n "${{ inputs.days_back }}" ]; then
            ARGS="$ARGS --days-back ${{ inputs.days_back }}"
          fi
          
          ARGS="$ARGS --output signals_${{ matrix.model.id }}.parquet"
          
          echo "Running: python run_inference.py $ARGS"
          python run_inference.py $ARGS
      
      - name: Upload signals artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: signals-${{ matrix.model.id }}-${{ github.run_id }}
          path: |
            signals_${{ matrix.model.id }}.parquet
            logs/
          retention-days: 30
      
      - name: Report status
        if: always()
        run: |
          echo "=== Model: ${{ matrix.model.id }} ==="
          echo "Paper mode: ${PAPER_MODE:-true}"
          echo "Dry run: ${{ inputs.dry_run || 'false' }}"
          
          if [ -f signals_${{ matrix.model.id }}.parquet ]; then
            echo "Signals file generated"
          else
            echo "No signals file (no trades or error)"
          fi

  notify:
    needs: inference
    runs-on: ubuntu-latest
    if: failure()
    
    steps:
      - name: Notify on failure
        run: |
          echo "One or more inference pipelines failed!"
          echo "Check the workflow logs for details."
